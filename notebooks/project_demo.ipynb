{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6235e3",
   "metadata": {},
   "source": [
    "# Section 1 : Gradient Checking\n",
    "\n",
    "Before training our network, it is important to verify that the **backpropagation implementation** is correct.  \n",
    "\n",
    "Gradient checking is a method to ensure that the **analytical gradients** computed by backpropagation match the **numerical gradients** estimated using finite differences.  \n",
    "\n",
    "We use the following approach:\n",
    "\n",
    "1. **Analytical Gradients:**  \n",
    "   - Computed using the `backward()` methods of our layers.  \n",
    "\n",
    "2. **Numerical Gradients:**  \n",
    "   - Approximated with the formula:  \n",
    "     \\[\n",
    "     \\frac{\\partial L}{\\partial W} \\approx \\frac{L(W + \\epsilon) - L(W - \\epsilon)}{2 \\epsilon}\n",
    "     \\]  \n",
    "   - Here, \\( \\epsilon \\) is a very small number (default \\( 1e-5 \\)).\n",
    "\n",
    "3. **Comparison:**  \n",
    "   - We calculate the maximum difference between analytical and numerical gradients.  \n",
    "   - If the difference is very small (e.g., less than \\(1e-6\\)), our backpropagation is likely correct.\n",
    "\n",
    "This step ensures the **core of our neural network library is functioning properly** before training on the XOR problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical:\n",
      " [[-0.00232373 -0.00274231 -0.00977339  0.01306674]\n",
      " [-0.00434791  0.01133584  0.002093    0.01870656]]\n",
      "Numerical:\n",
      " [[-0.00232373 -0.00274231 -0.00977339  0.01306674]\n",
      " [-0.00434791  0.01133584  0.002093    0.01870656]]\n",
      "Max Diff: 2.9750407866402373e-12\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the lib folder to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Test import\n",
    "from AROGO.layers import Dense\n",
    "from AROGO.activations import Tanh, Sigmoid\n",
    "from AROGO.losses import MSELoss\n",
    "from AROGO.optimizer import SGD\n",
    "from AROGO.network import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def numerical_gradient(model, X, Y, loss_fn, eps=1e-5):\n",
    "    grads_num = []\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, \"W\"):\n",
    "            dW_num = np.zeros_like(layer.W)\n",
    "\n",
    "            for i in range(layer.W.shape[0]):\n",
    "                for j in range(layer.W.shape[1]):\n",
    "                    W_old = layer.W[i, j]\n",
    "\n",
    "                    layer.W[i, j] = W_old + eps\n",
    "                    loss_plus = loss_fn.forward(Y, model.forward(X))\n",
    "\n",
    "                    layer.W[i, j] = W_old - eps\n",
    "                    loss_minus = loss_fn.forward(Y, model.forward(X))\n",
    "\n",
    "                    dW_num[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "                    layer.W[i, j] = W_old\n",
    "\n",
    "            grads_num.append(dW_num)\n",
    "\n",
    "    return grads_num\n",
    "\n",
    "\n",
    "# analytical gradients\n",
    "y_pred = model.forward(X)\n",
    "loss_fn.forward(Y, y_pred)\n",
    "grad = loss_fn.backward()\n",
    "model.backward(grad)\n",
    "\n",
    "# numerical gradients\n",
    "num = numerical_gradient(model, X, Y, loss_fn)\n",
    "\n",
    "dense1 = model.layers[0]\n",
    "print(\"Analytical:\\n\", dense1.dW)\n",
    "print(\"Numerical:\\n\", num[0])\n",
    "print(\"Max Diff:\", np.max(np.abs(dense1.dW - num[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d1f0f",
   "metadata": {},
   "source": [
    "# Section 2 : XOR Problem Using Our Custom Neural Network Library\n",
    "\n",
    "In this section, we demonstrate how to use the neural network library we implemented from scratch to solve the **XOR problem**.  \n",
    "  \n",
    "We train the network using:\n",
    "\n",
    "- **Mean Squared Error (MSE)** as the loss function  \n",
    "- **Stochastic Gradient Descent (SGD)** as the optimizer  \n",
    "\n",
    "The goal is to have the network correctly predict all four XOR outputs after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff6a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2539001108002241\n",
      "Epoch 2000, Loss: 0.006986212600061806\n",
      "Epoch 4000, Loss: 0.002426433168836391\n",
      "Epoch 6000, Loss: 0.00142500829139407\n",
      "Epoch 8000, Loss: 0.0009993669268076304\n",
      "Predictions after training:\n",
      "[[0.01332864]\n",
      " [0.97127211]\n",
      " [0.97049072]\n",
      " [0.03451367]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the lib folder to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Test import\n",
    "from AROGO.layers import Dense\n",
    "from AROGO.activations import Tanh, Sigmoid\n",
    "from AROGO.losses import MSELoss\n",
    "from AROGO.optimizer import SGD\n",
    "from AROGO.network import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, 4))\n",
    "model.add(Tanh())\n",
    "model.add(Dense(4, 1))\n",
    "model.add(Sigmoid())\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "optimizer = SGD(lr=0.1)\n",
    "\n",
    "model.train(X, Y, loss_fn, optimizer, epochs=5000)\n",
    "\n",
    "print(\"Predictions after training:\")\n",
    "print(model.forward(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfb8fa",
   "metadata": {},
   "source": [
    "# Section 5 : Baseline Comparison with TensorFlow/Keras\n",
    "\n",
    "To validate our custom neural network library, we implement the same **2-4-1 XOR network** using TensorFlow/Keras.  \n",
    "\n",
    "We will compare:\n",
    "\n",
    "1. **Ease of Implementation** – How quickly the network can be built and trained.  \n",
    "2. **Training Time** – Approximate time to converge.  \n",
    "3. **Final Predictions** – The output values for the XOR inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5304b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Predictions (Keras XOR):\n",
      "[[0.01457623]\n",
      " [0.9690686 ]\n",
      " [0.97115225]\n",
      " [0.03383316]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "Y = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "# Define the Keras model\n",
    "tf_model = Sequential([\n",
    "    Dense(4, input_dim=2, activation='tanh'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tf_model.compile(optimizer=SGD(learning_rate=0.1), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "tf_model.fit(X, Y, epochs=5000, verbose=0)\n",
    "\n",
    "# Predictions\n",
    "preds = tf_model.predict(X)\n",
    "print(\"Predictions (Keras XOR):\")\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b766c94",
   "metadata": {},
   "source": [
    "# comparison \n",
    "1- ease of implementaion :\n",
    "our lib has more ease of implementaion than tensor flow (keras) \n",
    "\n",
    "2-training time :\n",
    "our lib has less training time than tensor flow (keras) \n",
    "\n",
    "3-final predections :\n",
    " \n",
    "our lib results :\n",
    "[[0.01332864]\n",
    " [0.97127211]\n",
    " [0.97049072]\n",
    " [0.03451367]]\n",
    "\n",
    " tensor flow (keras) :\n",
    " [[0.01457623]\n",
    " [0.9690686 ]\n",
    " [0.97115225]\n",
    " [0.03383316]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
